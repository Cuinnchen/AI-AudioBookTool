# 数据模型

<cite>
**本文档中引用的文件**   
- [serverV2.py](file://serverV2.py)
- [config.json](file://config.json)
</cite>

## 目录
1. [简介](#简介)
2. [核心请求模型](#核心请求模型)
3. [章节JSON数据结构](#章节json数据结构)
4. [全局配置模型](#全局配置模型)
5. [数据模型使用场景与流程](#数据模型使用场景与流程)
6. [模型关联与数据转换](#模型关联与数据转换)

## 简介
本文档详细阐述了AI有声书工具后端服务（serverV2.py）中的核心数据模型。这些模型基于Pydantic构建，用于定义API接口的请求/响应数据结构，确保前后端数据交互的一致性与有效性。文档重点分析了`TTSRequestV2`、`SpliceRequest`、`MergeCharactersRequest`、`DeepAnalyzeRequest`、`UpdateConfigRequest`等关键请求模型，以及章节JSON的结构。同时，结合`config.json`文件，说明了全局配置模型的组成。通过本模型，系统实现了从文本到有声书的自动化生成流程。

**Section sources**
- [serverV2.py](file://serverV2.py#L57-L149)

## 核心请求模型
本节详细描述了在`serverV2.py`中定义的Pydantic请求模型，它们是API接口的输入契约。

### TTSRequestV2 模型
该模型用于驱动文本转语音（TTS）的生成请求，是语音生成流程的核心。

- **novel_name**: `str` - 小说项目的名称，用于定位项目目录。
- **chapter_name**: `str` - 当前处理的章节名称。
- **row_index**: `int` - 该文本在章节中的行索引，用于生成唯一音频文件名。
- **speaker**: `str` - 说话者角色名称（如“旁白”或角色名）。
- **timbre**: `str` - 为该说话者指定的音色名称。
- **tts_text**: `str` - 需要转换为语音的原始文本内容。
- **prompt_audio**: `str` - 参考音频的Base64编码字符串，用于语音克隆。
- **prompt_text**: `str` - 参考音频对应的文本，用于对齐。
- **inference_mode**: `str` - TTS模型的推理模式（如“Instruct”）。
- **instruct_text**: `Optional[str]` - 指令文本，用于指导语音生成的风格。
- **tts_model**: `Optional[str]` - 指定使用的TTS模型ID（如"cosyvoice_v2"），若未指定则使用默认模型。

此模型通过`/api/tts_v2`端点接收，驱动后端调用具体的TTS微服务生成音频。

**Section sources**
- [serverV2.py](file://serverV2.py#L75-L79)

### SpliceRequest 模型
该模型用于请求拼接单个章节的所有音频片段。

- **novel_name**: `str` - 小说项目的名称。
- **chapter_name**: `str` - 需要拼接的章节名称。
- **wav_files**: `List[str]` - （已弃用）此字段在代码中不再使用，后端会根据章节JSON文件自动生成需要拼接的文件列表。

该模型通过`/api/splice_audio`端点接收，后端会根据`chapters_json`目录下的JSON文件和`character_timbres.json`配置，智能地确定并拼接所有相关的WAV文件。

**Section sources**
- [serverV2.py](file://serverV2.py#L81-L82)

### MergeCharactersRequest 模型
该模型用于请求合并小说中的角色。

- **novel_name**: `str` - 小说项目的名称。
- **target_name**: `str` - 合并后保留的目标角色名称。
- **source_names**: `List[str]` - 需要被合并的源角色名称列表。
- **chapter_files**: `List[str]` - 需要处理的章节JSON文件列表。

当执行合并时，系统会修改所有指定章节JSON文件中的`speaker`字段，并尝试将源角色的WAV文件重命名为目标角色的文件名（如果音色相同），从而避免重新生成。

**Section sources**
- [serverV2.py](file://serverV2.py#L60-L63)

### DeepAnalyzeRequest 模型
该模型用于请求对角色进行深度信息补全。

- **novel_name**: `str` - 小说项目的名称。
- **character_name**: `str` - 需要分析的角色名称。
- **model_name**: `str` - 用于分析的LLM模型名称（如"gemini"）。

该请求会聚合该角色在所有已处理章节中的对话内容，然后调用LLM模型来补全其性别、年龄段和身份背景等信息。

**Section sources**
- [serverV2.py](file://serverV2.py#L125-L128)

### UpdateConfigRequest 模型
该模型用于更新小说项目的音色配置。

- **novel_name**: `str` - 小说项目的名称。
- **config_data**: `dict` - 包含角色与音色映射关系的字典，例如 `{"张三": "青年男声", "李四": "老年女声"}`。

该模型通过`/api/update_config`端点接收，将配置数据保存为项目目录下的`character_timbres.json`文件。

**Section sources**
- [serverV2.py](file://serverV2.py#L87-L88)

## 章节JSON数据结构
章节JSON文件是文本分析的输出结果，也是TTS生成的输入依据。其结构在`PROMPT_TEMPLATE`中有明确定义。

- **speaker**: `string` - 说话者姓名。"旁白"表示旁白，其他为角色名。
- **content**: `string` - 对话或旁白的具体内容。系统会自动清理只包含标点符号的内容。
- **tone**: `string` - 语气描述，如“正常”、“愤怒”、“开心”、“伤心”、“低声念叨”等。
- **intensity**: `int` (1-10) - 语气强度，数值越大表示情感越强烈。
- **delay**: `int` - 与上一句话之间的停顿时间（毫秒）。

**JSON示例:**
```json
[
  {
    "speaker": "旁白",
    "content": "夜色深沉，月光洒在古老的庭院中。",
    "tone": "正常",
    "intensity": 5,
    "delay": 500
  },
  {
    "speaker": "张三",
    "content": "李四，你终于来了！",
    "tone": "激动",
    "intensity": 8,
    "delay": 200
  }
]
```

该结构由LLM模型（如Gemini）根据`PROMPT_TEMPLATE`的指令生成，并通过`/api/process_single_chapter`等端点保存。

**Section sources**
- [serverV2.py](file://serverV2.py#L153-L185)

## 全局配置模型
`config.json`文件定义了系统的全局配置，包含多个子模型。

### general 模型
定义通用设置。

- **default_model**: `string` - 默认使用的LLM模型（如"gemini"）。
- **proxy**: `object` - 代理配置，包含`enabled`、`protocol`、`address`、`port`。
- **default_tts_model**: `string` - 默认使用的TTS模型（如"cosyvoice_v2"）。

### audio_export 模型
定义音频导出设置。

- **format**: `string` - 输出音频格式（如"mp3"）。
- **quality**: `string` - 输出音频质量（如"256k"）。

### tts_models 模型
定义可用的TTS模型及其配置。

- **模型ID (如 "cosyvoice_v2")**: `object`
  - **display_name**: `string` - 模型显示名称。
  - **endpoint**: `string` - TTS微服务的API端点地址。

### models 模型
定义可用的LLM模型及其配置。

- **模型ID (如 "gemini")**: `object`
  - **display_name**: `string` - 模型显示名称。
  - **model_name**: `string` - 模型在API中的实际名称。
  - **api_key**: `string` - 访问该模型的API密钥。
  - **max_chars**: `int` - 单次请求的最大字符数。
  - **use_proxy**: `bool` - 是否使用代理。

**JSON示例 (config.json):**
```json
{
  "general": {
    "default_model": "gemini",
    "proxy": {
      "enabled": true,
      "address": "127.0.0.1",
      "port": "1080"
    },
    "default_tts_model": "cosyvoice_v2"
  },
  "audio_export": {
    "format": "mp3",
    "quality": "256k"
  },
  "tts_models": {
    "cosyvoice_v2": {
      "display_name": "CosyVoice2",
      "endpoint": "http://127.0.0.1:5010/api/tts"
    }
  },
  "models": {
    "gemini": {
      "display_name": "Gemini",
      "model_name": "gemini-2.5-flash",
      "api_key": "your_api_key_here",
      "max_chars": 8000,
      "use_proxy": true
    }
  }
}
```

**Section sources**
- [config.json](file://config.json#L1-L45)
- [serverV2.py](file://serverV2.py#L195-L241)

## 数据模型使用场景与流程
这些数据模型协同工作，驱动整个有声书生成流程。

1.  **文本处理**: 用户上传小说文本，系统调用`ProcessSingleChapterRequest`模型处理章节，LLM模型根据`PROMPT_TEMPLATE`生成符合规范的章节JSON。
2.  **语音生成**: 系统遍历章节JSON中的每一项，构造`TTSRequestV2`对象，并调用`/api/tts_v2`端点。后端根据`config.json`中的`tts_models`配置，将请求转发给相应的TTS微服务，生成单句WAV文件。
3.  **音频拼接**: 当所有单句生成完毕，系统构造`SpliceRequest`对象，调用`/api/splice_audio`端点。后端根据章节JSON和`character_timbres.json`配置，将所有WAV文件按顺序拼接，并根据`audio_export`配置导出最终的音频文件。

**Section sources**
- [serverV2.py](file://serverV2.py#L1081-L1226)
- [serverV2.py](file://serverV2.py#L1728-L1855)
- [serverV2.py](file://serverV2.py#L1864-L1953)

## 模型关联与数据转换
各数据模型之间存在紧密的关联和数据转换过程。

- **章节JSON是核心枢纽**: `TTSRequestV2`的输入数据（`tts_text`, `speaker`等）直接来源于章节JSON文件中的每一项。
- **配置驱动行为**: `TTSRequestV2`中的`tts_model`字段和`inference_mode`等，其有效值和默认值由`config.json`中的`tts_models`和`general`模型定义。`SpliceRequest`的拼接逻辑依赖于`UpdateConfigRequest`保存的`character_timbres.json`配置。
- **数据转换**: `MergeCharactersRequest`会修改章节JSON文件中的`speaker`字段，并可能重命名WAV文件，实现了角色数据的合并。`DeepAnalyzeRequest`会读取章节JSON中的对话内容，将其作为上下文，通过LLM模型转换为角色的简介信息，并更新`character_profiles.json`。

这些模型共同构成了一个数据驱动的、可验证的API层，确保了整个系统数据流的完整性和一致性。

**Section sources**
- [serverV2.py](file://serverV2.py#L358-L479)
- [serverV2.py](file://serverV2.py#L767-L813)